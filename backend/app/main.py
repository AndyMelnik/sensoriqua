"""
Sensoriqua backend: grouping → objects → sensors → configured sensors + dashboard.
DSN: from JWT (Navixy App Connect), header X-Sensoriqua-DSN, or env SENSORIQUA_DSN.
Serves the frontend GUI from backend/static when that folder exists (e.g. after build).
"""
import os
import re
import uuid
from pathlib import Path
from typing import Any
from urllib.parse import urlparse

import psycopg.errors
from fastapi import Depends, FastAPI, Header, HTTPException, Query, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

from .auth import (
    RequestContext,
    create_token,
    get_request_context,
    is_app_connect_enabled,
    store_credentials,
)
from .db import (
    DEFAULT_DSN,
    get_conn,
    get_app_state_conn,
    app_state_uses_sqlite,
    app_state_table,
)

app = FastAPI(title="Sensoriqua", version="0.1.0")

# CORS: with credentials (Bearer tokens) do not use allow_origins=["*"].
# Set CORS_ORIGINS to comma-separated origins (e.g. https://app.example.com,https://admin.example.com).
_cors_origins_raw = os.environ.get("CORS_ORIGINS", "").strip()
CORS_ORIGINS = [o.strip() for o in _cors_origins_raw.split(",") if o.strip()]
if CORS_ORIGINS:
    app.add_middleware(
        CORSMiddleware,
        allow_origins=CORS_ORIGINS,
        allow_credentials=True,
        allow_methods=["GET", "POST", "PATCH", "DELETE", "OPTIONS"],
        allow_headers=["Authorization", "Content-Type", "X-Sensoriqua-DSN"],
    )
else:
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=False,
        allow_methods=["*"],
        allow_headers=["*"],
    )


# Iframe embedding: default allows any origin so the app can run inside an iframe.
# Set ALLOW_FRAME_ORIGINS to restrict (comma-separated), e.g. https://app.navixy.com
# Set ALLOW_FRAME_ORIGINS=deny to send X-Frame-Options: DENY (no embedding).
_ALLOW_FRAME_ORIGINS = os.environ.get("ALLOW_FRAME_ORIGINS", "").strip()


@app.middleware("http")
async def security_headers(request: Request, call_next):
    response = await call_next(request)
    response.headers["X-Content-Type-Options"] = "nosniff"
    if _ALLOW_FRAME_ORIGINS.lower() == "deny":
        response.headers["X-Frame-Options"] = "DENY"
    elif _ALLOW_FRAME_ORIGINS:
        origins = _ALLOW_FRAME_ORIGINS if _ALLOW_FRAME_ORIGINS == "*" else " ".join(o.strip() for o in _ALLOW_FRAME_ORIGINS.split(",") if o.strip())
        response.headers["Content-Security-Policy"] = f"frame-ancestors {origins}"
    else:
        response.headers["Content-Security-Policy"] = "frame-ancestors *"
    response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    return response


# Default user_id for testing when no auth
DEFAULT_USER_ID = int(os.environ.get("SENSORIQUA_USER_ID", "1"))

# When frontend is built into backend/static (e.g. on Render), we serve the GUI from /
_STATIC_DIR = Path(__file__).resolve().parent.parent / "static"
_SERVE_GUI = _STATIC_DIR.exists() and (_STATIC_DIR / "index.html").exists()


def _request_context(
    request: Request,
    x_sensoriqua_dsn: str | None = Header(None, alias="X-Sensoriqua-DSN"),
    user_id: int | None = Query(None),
) -> RequestContext:
    return get_request_context(
        request,
        x_sensoriqua_dsn=x_sensoriqua_dsn,
        user_id_query=user_id,
        default_user_id=DEFAULT_USER_ID,
        default_dsn=DEFAULT_DSN,
    )


# ---------- Pydantic models ----------

class GroupingQuery(BaseModel):
    type: str  # groups | tags | departments | garages
    search: str | None = None


class ObjectsFilter(BaseModel):
    group_ids: list[int] = []
    tag_ids: list[int] = []
    department_ids: list[int] = []
    garage_ids: list[int] = []
    sensor_type_ids: list[str] = []  # e.g. ["state", "tracking"] or sensor_type from sensor_description
    client_id: int | None = None  # optional tenant scope
    include_grouping_info: bool = False  # return group_label, tag_labels, department_label for UI grouping


class ConfiguredSensorCreate(BaseModel):
    object_id: int
    device_id: int
    sensor_input_label: str
    sensor_source: str = "input"  # input | state | tracking
    sensor_id: int | None = None
    sensor_label_custom: str
    min_threshold: float | None = None
    max_threshold: float | None = None
    multiplier: float | None = None


class ConfiguredSensorUpdate(BaseModel):
    sensor_label_custom: str | None = None
    min_threshold: float | None = None
    max_threshold: float | None = None
    multiplier: float | None = None


class DashboardPlaneCreate(BaseModel):
    configured_sensor_id: int
    position_index: int = 0


class SparklinesRequest(BaseModel):
    pairs: list[dict[str, Any]] = []  # [ {"device_id": 1, "sensor_input_label": "..." }, ... ]


class LatestValuesRequest(BaseModel):
    pairs: list[dict[str, Any]] = []


class SensorHistoryRequest(BaseModel):
    device_id: int
    sensor_input_label: str
    sensor_source: str = "input"  # input | state | tracking
    hours: int = 1  # 1, 4, 12, or 24


class DashboardOrderRequest(BaseModel):
    order: list[dict[str, Any]] = []  # [ {"dashboard_plane_id": 1, "position_index": 0 }, ... ]


class AuthLoginRequest(BaseModel):
    """Navixy App Connect: payload from middleware."""
    email: str
    iotDbUrl: str
    userDbUrl: str
    role: str = "admin"


# ---------- Navixy App Connect: auth endpoint ----------

_ALLOWED_DSN_SCHEMES = ("postgresql", "postgres")
# Block private/internal hosts when validating login DSNs (SSRF mitigation).
# Set ALLOW_PRIVATE_DSN=1 only in trusted environments (e.g. backend runs inside Navixy).
_PRIVATE_HOST_PATTERN = re.compile(
    r"^(localhost|127\.|10\.|172\.(1[6-9]|2\d|3[01])\.|192\.168\.|169\.254\.|::1)",
    re.IGNORECASE,
)
ALLOW_PRIVATE_DSN = os.environ.get("ALLOW_PRIVATE_DSN", "").strip().lower() in ("1", "true", "yes")


def _validate_dsn_for_login(dsn: str, name: str) -> None:
    """Ensure DSN is a Postgres URL and optionally block private hosts (SSRF)."""
    if not dsn or not dsn.strip():
        raise HTTPException(status_code=400, detail=f"{name} is required")
    try:
        p = urlparse(dsn.strip())
    except Exception:
        raise HTTPException(status_code=400, detail=f"Invalid {name}")
    scheme = (p.scheme or "").lower()
    if scheme not in _ALLOWED_DSN_SCHEMES:
        raise HTTPException(
            status_code=400,
            detail=f"{name} must be a PostgreSQL URL (postgresql:// or postgres://)",
        )
    if not ALLOW_PRIVATE_DSN:
        host = (p.hostname or "").strip()
        if host and _PRIVATE_HOST_PATTERN.match(host):
            raise HTTPException(
                status_code=400,
                detail=f"{name} must not point to localhost or private network",
            )


@app.post("/api/auth/login")
def auth_login(body: AuthLoginRequest):
    """
    Navixy App Connect: middleware calls this with user info and DB URLs.
    Returns JWT; store iotDbUrl/userDbUrl server-side for this user.
    Requires JWT_SECRET (min 32 chars) in env.
    DSNs must be PostgreSQL and must not target localhost/private IPs (SSRF mitigation).
    """
    if not is_app_connect_enabled():
        raise HTTPException(
            status_code=501,
            detail="Navixy App Connect not configured (set JWT_SECRET with at least 32 characters)",
        )
    if not body.email or not body.iotDbUrl or not body.userDbUrl:
        raise HTTPException(
            status_code=400,
            detail="Missing required fields: email, iotDbUrl, userDbUrl",
        )
    _validate_dsn_for_login(body.iotDbUrl, "iotDbUrl")
    _validate_dsn_for_login(body.userDbUrl, "userDbUrl")
    user_id = str(uuid.uuid4())
    store_credentials(user_id, body.iotDbUrl, body.userDbUrl)
    token = create_token(user_id, body.email, body.role or "admin")
    return {
        "success": True,
        "user": {"id": user_id, "email": body.email, "role": body.role or "admin"},
        "token": token,
    }


# ---------- Config (DSN at top for testing) ----------

@app.get("/api/config")
def get_config(ctx: RequestContext = Depends(_request_context)):
    """Return default DSN (masked) for display at top of UI. Password hidden."""
    dsn = ctx.dsn
    if "@" in dsn and "://" in dsn:
        try:
            from urllib.parse import urlparse
            p = urlparse(dsn)
            if p.password:
                netloc = p.hostname or ""
                if p.port:
                    netloc += f":{p.port}"
                dsn_display = f"{p.scheme}://{p.username}:***@{netloc}{p.path or '/'}"
            else:
                dsn_display = dsn
        except Exception:
            dsn_display = dsn
    else:
        dsn_display = dsn
    return {"dsn_placeholder": dsn_display, "default_user_id": ctx.user_id}


# ---------- Groupings ----------

@app.get("/api/groupings")
def list_groupings(
    type: str = Query(..., description="groups | tags | departments | garages | sensor_types"),
    search: str | None = Query(None),
    ctx: RequestContext = Depends(_request_context),
):
    schema = "raw_business_data"
    schema_tel = "raw_telematics_data"
    dsn = ctx.dsn
    with get_conn(dsn) as conn:
        if type == "sensor_types":
            # Distinct sensor_type from sensor_description + fixed "state" and "tracking"
            out_sensor_types: list[dict[str, Any]] = []
            try:
                cur = conn.execute(
                    f"""
                    SELECT DISTINCT sensor_type AS id
                    FROM {schema}.sensor_description
                    WHERE sensor_type IS NOT NULL AND sensor_type != ''
                    ORDER BY sensor_type
                    """
                )
                for r in cur.fetchall():
                    st = str(r["id"])
                    out_sensor_types.append({"id": st, "label": st})
            except Exception:
                pass
            for sid, label in [("state", "State"), ("tracking", "Tracking")]:
                if not search or search.lower() in label.lower() or search.lower() in sid.lower():
                    out_sensor_types.append({"id": sid, "label": label})
            return out_sensor_types
        if type == "groups":
            q = f"""
                SELECT group_id AS id, group_label AS label
                FROM {schema}.groups
                WHERE 1=1
            """
            if search:
                q += " AND group_label ILIKE %(search)s"
            q += " ORDER BY group_label"
            cur = conn.execute(q, {"search": f"%{search}%" if search else None})
        elif type == "tags":
            q = f"""
                SELECT tag_id AS id, tag_label AS label
                FROM {schema}.tags
                WHERE 1=1
            """
            if search:
                q += " AND tag_label ILIKE %(search)s"
            q += " ORDER BY tag_label"
            cur = conn.execute(q, {"search": f"%{search}%" if search else None})
        elif type == "departments":
            q = f"""
                SELECT department_id AS id, department_label AS label
                FROM {schema}.departments
                WHERE 1=1
            """
            if search:
                q += " AND department_label ILIKE %(search)s"
            q += " ORDER BY department_label"
            cur = conn.execute(q, {"search": f"%{search}%" if search else None})
        elif type == "garages":
            # Schema: garages has organization_label, no garage_label
            q = f"""
                SELECT garage_id AS id, COALESCE(organization_label, 'Garage ' || garage_id::text) AS label
                FROM {schema}.garages
                WHERE 1=1
            """
            if search:
                q += " AND organization_label ILIKE %(search)s"
            q += " ORDER BY label"
            cur = conn.execute(q, {"search": f"%{search}%" if search else None})
        else:
            raise HTTPException(400, "type must be groups|tags|departments|garages|sensor_types")
        rows = cur.fetchall()
    return [dict(r) for r in rows]


# ---------- Objects (filtered by groupings) ----------

@app.post("/api/objects")
def list_objects(
    body: ObjectsFilter,
    ctx: RequestContext = Depends(_request_context),
):
    """Return objects matching any of the selected groupings (OR across types).
    Schema: objects.group_id -> groups; tag_links(entity_id=object_id, entity_type=int, tag_id) -> tags.
    If grouping_info query fails, falls back to plain list so UI always gets objects.
    """
    dsn = ctx.dsn
    schema = "raw_business_data"
    conditions = []
    params: dict[str, Any] = {}
    if body.group_ids:
        conditions.append("o.group_id = ANY(%(group_ids)s)")
        params["group_ids"] = body.group_ids
    if body.tag_ids:
        entity_type_object = int(os.environ.get("SENSORIQUA_TAG_ENTITY_TYPE_OBJECT", "1"))
        conditions.append("""
            EXISTS (
                SELECT 1 FROM raw_business_data.tag_links tl
                WHERE tl.entity_id = o.object_id
                  AND tl.entity_type = %(tag_entity_type)s
                  AND tl.tag_id = ANY(%(tag_ids)s)
            )
        """)
        params["tag_entity_type"] = entity_type_object
        params["tag_ids"] = body.tag_ids
    if body.department_ids:
        conditions.append("""
            EXISTS (
                SELECT 1 FROM raw_business_data.employees e
                WHERE e.object_id = o.object_id
                  AND e.department_id = ANY(%(department_ids)s)
            )
        """)
        params["department_ids"] = body.department_ids
    if body.garage_ids:
        conditions.append("""
            EXISTS (
                SELECT 1 FROM raw_business_data.vehicles v
                WHERE v.object_id = o.object_id
                  AND v.garage_id = ANY(%(garage_ids)s)
            )
        """)
        params["garage_ids"] = body.garage_ids

    if body.sensor_type_ids:
        # Objects whose device has at least one sensor of any of the selected types
        # "state" -> device in states; "tracking" -> device in tracking_data_core; else -> sensor_type from sensor_description
        type_conds = []
        state_ids = [t for t in body.sensor_type_ids if t == "state"]
        tracking_ids = [t for t in body.sensor_type_ids if t == "tracking"]
        other_ids = [t for t in body.sensor_type_ids if t not in ("state", "tracking")]
        if state_ids:
            type_conds.append(f"o.device_id IN (SELECT DISTINCT device_id FROM {schema_tel}.states)")
        if tracking_ids:
            type_conds.append(f"o.device_id IN (SELECT DISTINCT device_id FROM {schema_tel}.tracking_data_core)")
        if other_ids:
            type_conds.append(
                f"o.device_id IN (SELECT DISTINCT device_id FROM {schema}.sensor_description WHERE sensor_type = ANY(%(sensor_type_ids)s))"
            )
            params["sensor_type_ids"] = other_ids
        if type_conds:
            conditions.append("(" + " OR ".join(type_conds) + ")")

    out: list[dict[str, Any]] = []
    try:
        with get_conn(dsn) as conn:
            if body.include_grouping_info:
                _entity_type_obj = int(os.environ.get("SENSORIQUA_TAG_ENTITY_TYPE_OBJECT", "1"))
                params["_tag_entity_type"] = _entity_type_obj
                sel = f"""o.object_id AS id, o.object_label AS label, o.device_id, o.group_id,
                    g.group_label,
                    (SELECT COALESCE(array_agg(t.tag_label) FILTER (WHERE t.tag_label IS NOT NULL), '{{}}')
                     FROM {schema}.tag_links tl LEFT JOIN {schema}.tags t ON t.tag_id = tl.tag_id
                     WHERE tl.entity_id = o.object_id AND tl.entity_type = %(_tag_entity_type)s) AS tag_labels,
                    (SELECT d.department_label FROM {schema}.employees e
                     JOIN {schema}.departments d ON d.department_id = e.department_id
                     WHERE e.object_id = o.object_id LIMIT 1) AS department_label"""
                joins = f"LEFT JOIN {schema}.groups g ON g.group_id = o.group_id"
                if not conditions:
                    where = "o.is_deleted = false"
                    if body.client_id is not None:
                        where += " AND o.client_id = %(client_id)s"
                        params["client_id"] = body.client_id
                    sql = f"""
                        SELECT {sel}
                        FROM {schema}.objects o
                        {joins}
                        WHERE {where}
                        ORDER BY o.object_label
                    """
                else:
                    where = " AND ".join(conditions)
                    where = f"o.is_deleted = false AND ({where})"
                    if body.client_id is not None:
                        where += " AND o.client_id = %(client_id)s"
                        params["client_id"] = body.client_id
                    sql = f"""
                        SELECT DISTINCT ON (o.object_id) {sel}
                        FROM {schema}.objects o
                        {joins}
                        WHERE {where}
                        ORDER BY o.object_id, o.object_label
                    """
            else:
                if not conditions:
                    where = "o.is_deleted = false"
                    if body.client_id is not None:
                        where += " AND o.client_id = %(client_id)s"
                        params["client_id"] = body.client_id
                    sql = f"""
                        SELECT o.object_id AS id, o.object_label AS label, o.device_id
                        FROM {schema}.objects o
                        WHERE {where}
                        ORDER BY o.object_label
                    """
                else:
                    where = " AND ".join(conditions)
                    where = f"o.is_deleted = false AND ({where})"
                    if body.client_id is not None:
                        where += " AND o.client_id = %(client_id)s"
                        params["client_id"] = body.client_id
                    sql = f"""
                        SELECT DISTINCT o.object_id AS id, o.object_label AS label, o.device_id
                        FROM {schema}.objects o
                        WHERE {where}
                        ORDER BY o.object_label
                    """
            cur = conn.execute(sql, params)
            rows = cur.fetchall()
        out = [dict(r) for r in rows]
        if body.include_grouping_info and out:
            for row in out:
                if "tag_labels" in row and hasattr(row["tag_labels"], "__iter__") and not isinstance(row["tag_labels"], str):
                    row["tag_labels"] = list(row["tag_labels"]) if row["tag_labels"] is not None else []
                else:
                    row["tag_labels"] = getattr(row.get("tag_labels"), "__iter__", None) and list(row["tag_labels"]) or []
    except Exception:
        with get_conn(dsn) as conn:
            if not conditions:
                where = "o.is_deleted = false"
                if body.client_id is not None:
                    where += " AND o.client_id = %(client_id)s"
                    params["client_id"] = body.client_id
                sql = f"""
                    SELECT o.object_id AS id, o.object_label AS label, o.device_id
                    FROM {schema}.objects o
                    WHERE {where}
                    ORDER BY o.object_label
                """
            else:
                where = " AND ".join(conditions)
                where = f"o.is_deleted = false AND ({where})"
                if body.client_id is not None:
                    where += " AND o.client_id = %(client_id)s"
                    params["client_id"] = body.client_id
                sql = f"""
                    SELECT DISTINCT o.object_id AS id, o.object_label AS label, o.device_id
                    FROM {schema}.objects o
                    WHERE {where}
                    ORDER BY o.object_label
                """
            cur = conn.execute(sql, params)
            rows = cur.fetchall()
        for r in rows:
            row = dict(r)
            row["group_id"] = None
            row["group_label"] = None
            row["tag_labels"] = []
            row["department_label"] = None
            out.append(row)
    return out


# Telematics: tracking_data_core value columns (no sensor_name column; use column names)
TRACKING_DATA_CORE_SIGNALS = [
    "latitude", "longitude", "speed", "altitude", "satellites", "hdop", "gps_fix_type", "event_id",
]


# ---------- Sensors per object ----------

@app.get("/api/objects/{object_id:int}/sensors")
def list_sensors_for_object(
    object_id: int,
    search: str | None = Query(None),
    include_type_and_params: bool = Query(True),
    ctx: RequestContext = Depends(_request_context),
):
    """Combined distinct sensor list from raw_telematics_data: inputs (sensor_name), states (state_name), tracking_data_core (column names)."""
    dsn = ctx.dsn
    schema_biz = "raw_business_data"
    schema_tel = "raw_telematics_data"
    with get_conn(dsn) as conn:
        cur = conn.execute(
            f"SELECT device_id FROM {schema_biz}.objects WHERE object_id = %s",
            (object_id,),
        )
        row = cur.fetchone()
        if not row:
            raise HTTPException(404, "Object not found")
        device_id = row["device_id"]
        if not device_id:
            return []
        out: list[dict[str, Any]] = []

        # 1) Distinct sensor_name from inputs (join sensor_description for label/type)
        try:
            cur = conn.execute(
                f"""
                SELECT DISTINCT i.sensor_name AS input_label
                FROM {schema_tel}.inputs i
                WHERE i.device_id = %s
                ORDER BY i.sensor_name
                """,
                (device_id,),
            )
            input_names = [r["input_label"] for r in cur.fetchall()]
        except Exception:
            input_names = []
        sd_map: dict[str, dict] = {}
        if input_names:
            try:
                cur = conn.execute(
                    f"""
                    SELECT sensor_id, sensor_label, input_label, sensor_type, sensor_units, units_type
                    FROM {schema_biz}.sensor_description
                    WHERE device_id = %s AND input_label = ANY(%s)
                    """,
                    (device_id, input_names),
                )
                for r in cur.fetchall():
                    sd_map[r["input_label"]] = dict(r)
            except Exception:
                pass
        units_lookup: dict[int, str] = {}
        try:
            cur = conn.execute(
                f"""
                SELECT key, type, description FROM {schema_biz}.description_parametrs
                WHERE type = 'sensor_description_units_type'
                """
            )
            units_lookup = {r["key"]: r["description"] for r in cur.fetchall()}
        except Exception:
            pass
        for name in input_names:
            sd = sd_map.get(name) or {}
            dp = []
            if sd.get("units_type") is not None and sd["units_type"] in units_lookup:
                dp = [{"name": "units_type", "value": units_lookup[sd["units_type"]]}]
            out.append({
                "source": "input",
                "sensor_id": sd.get("sensor_id"),
                "input_label": name,
                "label": (sd.get("sensor_label") or name) or "",
                "sensor_type": sd.get("sensor_type"),
                "sensor_units": sd.get("sensor_units"),
                "description_parameters": dp,
            })

        # 2) Distinct state_name from states
        try:
            cur = conn.execute(
                f"""
                SELECT DISTINCT state_name AS input_label
                FROM {schema_tel}.states
                WHERE device_id = %s
                ORDER BY state_name
                """,
                (device_id,),
            )
            for r in cur.fetchall():
                name = r["input_label"]
                out.append({
                    "source": "state",
                    "sensor_id": None,
                    "input_label": name,
                    "label": name,
                    "sensor_type": "state",
                    "sensor_units": None,
                    "description_parameters": [],
                })
        except Exception:
            pass

        # 3) tracking_data_core: fixed list of value column names
        for name in TRACKING_DATA_CORE_SIGNALS:
            out.append({
                "source": "tracking",
                "sensor_id": None,
                "input_label": name,
                "label": name,
                "sensor_type": "tracking_data_core",
                "sensor_units": None,
                "description_parameters": [],
            })

    if search:
        search_lower = search.lower()
        out = [
            x for x in out
            if search_lower in (x.get("label") or "").lower()
            or search_lower in (x.get("input_label") or "").lower()
            or search_lower in (x.get("sensor_type") or "").lower()
        ]
    return out


# ---------- Configured sensors CRUD ----------

@app.get("/api/configured-sensors")
def list_configured_sensors(ctx: RequestContext = Depends(_request_context)):
    uid = ctx.user_id
    dsn = ctx.dsn
    cfg = app_state_table("configured_sensors")
    use_sqlite = ctx.app_state_dsn is None and app_state_uses_sqlite()
    try:
        with get_app_state_conn(dsn, override_dsn=ctx.app_state_dsn) as conn:
            if use_sqlite:
                cur = conn.execute(
                    f"""
                    SELECT configured_sensor_id, object_id, device_id, sensor_input_label,
                           sensor_source, sensor_id, sensor_label_custom, min_threshold, max_threshold,
                           multiplier, created_at
                    FROM {cfg}
                    WHERE user_id = %s AND is_active = 1
                    ORDER BY created_at DESC
                    """,
                    (uid,),
                )
                rows = cur.fetchall()
                if not rows:
                    return []
                object_ids = list({r["object_id"] for r in rows})
                with get_conn(dsn) as pg:
                    cur2 = pg.execute(
                        "SELECT object_id, object_label FROM raw_business_data.objects WHERE object_id = ANY(%s)",
                        (object_ids,),
                    )
                    labels = {r["object_id"]: r["object_label"] for r in cur2.fetchall()}
                for r in rows:
                    r["object_label"] = labels.get(r["object_id"])
                return [dict(r) for r in rows]
            try:
                cur = conn.execute(
                    f"""
                    SELECT c.configured_sensor_id, c.object_id, c.device_id, c.sensor_input_label,
                           c.sensor_source, c.sensor_id, c.sensor_label_custom, c.min_threshold, c.max_threshold,
                           c.multiplier, c.created_at,
                           o.object_label
                    FROM {cfg} c
                    JOIN raw_business_data.objects o ON o.object_id = c.object_id
                    WHERE c.user_id = %s AND c.is_active = true
                    ORDER BY c.created_at DESC
                    """,
                    (uid,),
                )
                rows = cur.fetchall()
            except psycopg.errors.UndefinedColumn:
                cur = conn.execute(
                    f"""
                    SELECT c.configured_sensor_id, c.object_id, c.device_id, c.sensor_input_label,
                           c.sensor_id, c.sensor_label_custom, c.min_threshold, c.max_threshold,
                           c.created_at,
                           o.object_label
                    FROM {cfg} c
                    JOIN raw_business_data.objects o ON o.object_id = c.object_id
                    WHERE c.user_id = %s AND c.is_active = true
                    ORDER BY c.created_at DESC
                    """,
                    (uid,),
                )
                rows = cur.fetchall()
                rows = [{**dict(r), "sensor_source": "input", "multiplier": None} for r in rows]
            return [dict(r) for r in rows]
    except psycopg.errors.UndefinedTable:
        return []


@app.post("/api/configured-sensors")
def add_configured_sensor(
    body: ConfiguredSensorCreate,
    ctx: RequestContext = Depends(_request_context),
):
    uid = ctx.user_id
    if body.min_threshold is not None and body.max_threshold is not None and body.min_threshold >= body.max_threshold:
        raise HTTPException(400, "MIN must be less than MAX")
    dsn = ctx.dsn
    source = (body.sensor_source or "input").strip().lower()
    if source not in ("input", "state", "tracking"):
        source = "input"
    cfg = app_state_table("configured_sensors")
    use_sqlite = ctx.app_state_dsn is None and app_state_uses_sqlite()
    try:
        with get_app_state_conn(dsn, override_dsn=ctx.app_state_dsn) as conn:
            row = None
            if use_sqlite:
                cur = conn.execute(
                    f"""
                    INSERT INTO {cfg}
                    (user_id, object_id, device_id, sensor_input_label, sensor_source, sensor_id, sensor_label_custom, min_threshold, max_threshold, multiplier)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    RETURNING configured_sensor_id, object_id, device_id, sensor_input_label, sensor_source, sensor_label_custom, min_threshold, max_threshold, multiplier, created_at
                    """,
                    (
                        uid,
                        body.object_id,
                        body.device_id,
                        body.sensor_input_label,
                        source,
                        body.sensor_id,
                        body.sensor_label_custom,
                        body.min_threshold,
                        body.max_threshold,
                        body.multiplier,
                    ),
                )
                row = cur.fetchone()
            else:
                try:
                    cur = conn.execute(
                        f"""
                        INSERT INTO {cfg}
                        (user_id, object_id, device_id, sensor_input_label, sensor_source, sensor_id, sensor_label_custom, min_threshold, max_threshold, multiplier)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        RETURNING configured_sensor_id, object_id, device_id, sensor_input_label, sensor_source, sensor_label_custom, min_threshold, max_threshold, multiplier, created_at
                        """,
                        (
                            uid,
                            body.object_id,
                            body.device_id,
                            body.sensor_input_label,
                            source,
                            body.sensor_id,
                            body.sensor_label_custom,
                            body.min_threshold,
                            body.max_threshold,
                            body.multiplier,
                        ),
                    )
                    row = cur.fetchone()
                except psycopg.errors.UndefinedColumn:
                    cur = conn.execute(
                        f"""
                        INSERT INTO {cfg}
                        (user_id, object_id, device_id, sensor_input_label, sensor_id, sensor_label_custom, min_threshold, max_threshold)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                        RETURNING configured_sensor_id, object_id, device_id, sensor_input_label, sensor_label_custom, min_threshold, max_threshold, created_at
                        """,
                        (
                            uid,
                            body.object_id,
                            body.device_id,
                            body.sensor_input_label,
                            body.sensor_id,
                            body.sensor_label_custom,
                            body.min_threshold,
                            body.max_threshold,
                        ),
                    )
                    row = cur.fetchone()
                    if row is not None:
                        row = dict(row)
                        row["sensor_source"] = "input"
                        row["multiplier"] = None
            if row is None:
                raise HTTPException(status_code=500, detail="INSERT returned no row")
            conn.commit()
            # Attach object_label from main DB
            with get_conn(dsn) as pg:
                cur2 = pg.execute(
                    "SELECT object_label FROM raw_business_data.objects WHERE object_id = %s",
                    (body.object_id,),
                )
                ob = cur2.fetchone()
            out = dict(row)
            out["object_label"] = ob["object_label"] if ob else None
            return out
    except HTTPException:
        raise
    except psycopg.errors.UndefinedTable as e:
        if "configured_sensors" in str(e) or "app_sensoriqua" in str(e):
            raise HTTPException(
                status_code=503,
                detail="Configured sensors table not found. Add to backend/.env: SENSORIQUA_APP_STATE_DSN=sqlite:///./sensoriqua_state.db to use local storage without DB migrations (no app_sensoriqua schema required).",
            )
        raise HTTPException(status_code=500, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.patch("/api/configured-sensors/{configured_sensor_id:int}")
def update_configured_sensor(
    configured_sensor_id: int,
    body: ConfiguredSensorUpdate,
    ctx: RequestContext = Depends(_request_context),
):
    uid = ctx.user_id
    if body.min_threshold is not None and body.max_threshold is not None and body.min_threshold >= body.max_threshold:
        raise HTTPException(400, "MIN must be less than MAX")
    dsn = ctx.dsn
    cfg = app_state_table("configured_sensors")
    use_sqlite = ctx.app_state_dsn is None and app_state_uses_sqlite()
    updated_at = "datetime('now')" if use_sqlite else "now()"
    with get_app_state_conn(dsn, override_dsn=ctx.app_state_dsn) as conn:
        updates = []
        params: list[Any] = []
        payload = body.model_dump(exclude_unset=True)
        if "sensor_label_custom" in payload:
            updates.append("sensor_label_custom = %s")
            params.append(body.sensor_label_custom)
        if "min_threshold" in payload:
            updates.append("min_threshold = %s")
            params.append(body.min_threshold)
        if "max_threshold" in payload:
            updates.append("max_threshold = %s")
            params.append(body.max_threshold)
        if "multiplier" in payload:
            updates.append("multiplier = %s")
            params.append(body.multiplier)
        if not updates:
            raise HTTPException(400, "No fields to update")
        updates.append(f"updated_at = {updated_at}")
        params.extend([configured_sensor_id, uid])
        cur = conn.execute(
            f"UPDATE {cfg} SET {', '.join(updates)} WHERE configured_sensor_id = %s AND user_id = %s RETURNING configured_sensor_id",
            params,
        )
        if cur.rowcount == 0:
            raise HTTPException(404, "Configured sensor not found")
        conn.commit()
    return {"ok": True}


@app.delete("/api/configured-sensors/{configured_sensor_id:int}")
def delete_configured_sensor(
    configured_sensor_id: int,
    ctx: RequestContext = Depends(_request_context),
):
    uid = ctx.user_id
    dsn = ctx.dsn
    cfg = app_state_table("configured_sensors")
    use_sqlite = ctx.app_state_dsn is None and app_state_uses_sqlite()
    updated_at = "datetime('now')" if use_sqlite else "now()"
    is_active_val = 0 if use_sqlite else False
    with get_app_state_conn(dsn, override_dsn=ctx.app_state_dsn) as conn:
        cur = conn.execute(
            f"UPDATE {cfg} SET is_active = %s, updated_at = {updated_at} WHERE configured_sensor_id = %s AND user_id = %s",
            (is_active_val, configured_sensor_id, uid),
        )
        conn.commit()
        if cur.rowcount == 0:
            raise HTTPException(404, "Configured sensor not found")
    return {"ok": True}


# ---------- Sparklines (batch last hour) ----------

def _series_key(device_id: int, label: str, source: str) -> str:
    return f"{device_id}:{source}:{label}"


@app.post("/api/sparklines")
def batch_sparklines(
    body: SparklinesRequest,
    ctx: RequestContext = Depends(_request_context),
):
    """Body: { "pairs": [ { "device_id", "sensor_input_label", "sensor_source"?: "input"|"state"|"tracking" }, ... ] }
    Returns: { "series": { "device_id:source:sensor_input_label": [ { "ts", "value" }, ... ] } }
    """
    pairs = body.pairs or []
    if not pairs:
        return {"series": {}}
    normalized = []
    for p in pairs:
        src = (p.get("sensor_source") or "input").strip().lower()
        if src not in ("input", "state", "tracking"):
            src = "input"
        normalized.append((p["device_id"], p["sensor_input_label"], src))
    dsn = ctx.dsn
    series: dict[str, list[dict]] = {}
    with get_conn(dsn) as conn:
        cur = conn.execute(
            "SELECT EXISTS (SELECT 1 FROM pg_proc p JOIN pg_namespace n ON p.pronamespace = n.oid WHERE n.nspname = 'timescale' AND p.proname = 'time_bucket') AS has_tb"
        )
        row_tb = cur.fetchone()
        has_tb = bool(row_tb and row_tb.get("has_tb"))

        # --- inputs ---
        input_keys = [(d, l) for (d, l, s) in normalized if s == "input"]
        if input_keys:
            placeholders = ",".join(["(%s,%s)"] * len(input_keys))
            flat = [x for k in input_keys for x in k]
            bucket_expr = "time_bucket('1 minute', i.device_time)" if has_tb else "date_trunc('minute', i.device_time)"
            sql = f"""
                WITH cfg(device_id, sensor_name) AS (VALUES {placeholders}),
                series AS (
                    SELECT i.device_id, i.sensor_name, {bucket_expr} AS bucket_ts,
                           avg(NULLIF(i.value,'')::numeric) AS value
                    FROM raw_telematics_data.inputs i
                    JOIN cfg ON cfg.device_id = i.device_id AND cfg.sensor_name = i.sensor_name
                    WHERE i.device_time >= now() - interval '1 hour'
                    GROUP BY i.device_id, i.sensor_name, {bucket_expr}
                )
                SELECT device_id, sensor_name, bucket_ts AS ts, value FROM series ORDER BY device_id, sensor_name, ts
            """
            cur = conn.execute(sql, flat)
            for r in cur.fetchall():
                key = _series_key(r["device_id"], r["sensor_name"], "input")
                if key not in series:
                    series[key] = []
                series[key].append({"ts": r["ts"].isoformat() if hasattr(r["ts"], "isoformat") else str(r["ts"]), "value": float(r["value"]) if r["value"] is not None else None})

        # --- states ---
        state_keys = [(d, l) for (d, l, s) in normalized if s == "state"]
        if state_keys:
            placeholders = ",".join(["(%s,%s)"] * len(state_keys))
            flat = [x for k in state_keys for x in k]
            bucket_expr = "time_bucket('1 minute', s.device_time)" if has_tb else "date_trunc('minute', s.device_time)"
            sql = f"""
                WITH cfg(device_id, state_name) AS (VALUES {placeholders}),
                series AS (
                    SELECT s.device_id, s.state_name AS sensor_name, {bucket_expr} AS bucket_ts,
                           avg(NULLIF(s.value,'')::numeric) AS value
                    FROM raw_telematics_data.states s
                    JOIN cfg ON cfg.device_id = s.device_id AND cfg.state_name = s.state_name
                    WHERE s.device_time >= now() - interval '1 hour'
                    GROUP BY s.device_id, s.state_name, {bucket_expr}
                )
                SELECT device_id, sensor_name, bucket_ts AS ts, value FROM series ORDER BY device_id, sensor_name, ts
            """
            cur = conn.execute(sql, flat)
            for r in cur.fetchall():
                key = _series_key(r["device_id"], r["sensor_name"], "state")
                if key not in series:
                    series[key] = []
                series[key].append({"ts": r["ts"].isoformat() if hasattr(r["ts"], "isoformat") else str(r["ts"]), "value": float(r["value"]) if r["value"] is not None else None})

        # --- tracking_data_core (one query per column, whitelisted) ---
        tracking_pairs = [(d, l) for (d, l, s) in normalized if s == "tracking" and l in TRACKING_DATA_CORE_SIGNALS]
        if tracking_pairs:
            col_to_devices: dict[str, list[int]] = {}
            for d, col in tracking_pairs:
                col_to_devices.setdefault(col, []).append(d)
            for col in col_to_devices:
                device_ids = list(dict.fromkeys(col_to_devices[col]))
                placeholders = ",".join(["%s"] * len(device_ids))
                bucket_expr = "time_bucket('1 minute', t.device_time)" if has_tb else "date_trunc('minute', t.device_time)"
                # Safe: col is from TRACKING_DATA_CORE_SIGNALS
                sql = f"""
                    SELECT t.device_id, {bucket_expr} AS bucket_ts,
                           avg((t.{col})::numeric) AS value
                    FROM raw_telematics_data.tracking_data_core t
                    WHERE t.device_id IN ({placeholders}) AND t.device_time >= now() - interval '1 hour'
                    GROUP BY t.device_id, {bucket_expr}
                    ORDER BY t.device_id, bucket_ts
                """
                cur = conn.execute(sql, device_ids)
                for r in cur.fetchall():
                    key = _series_key(r["device_id"], col, "tracking")
                    if key not in series:
                        series[key] = []
                    series[key].append({"ts": r["bucket_ts"].isoformat() if hasattr(r["bucket_ts"], "isoformat") else str(r["bucket_ts"]), "value": float(r["value"]) if r["value"] is not None else None})

    return {"series": series}


# ---------- Sensor history (single sensor, configurable duration) ----------

@app.post("/api/sensor-history")
def sensor_history(
    body: SensorHistoryRequest,
    ctx: RequestContext = Depends(_request_context),
):
    """Body: { "device_id", "sensor_input_label", "sensor_source"?: "input"|"state"|"tracking", "hours": 1|4|12|24 }.
    Returns: { "series": [ { "ts", "value" }, ... ] } for the given sensor over the last N hours.
    """
    hours = body.hours
    if hours not in (1, 4, 12, 24):
        raise HTTPException(status_code=400, detail="hours must be 1, 4, 12, or 24")
    src = (body.sensor_source or "input").strip().lower()
    if src not in ("input", "state", "tracking"):
        src = "input"
    if src == "tracking" and body.sensor_input_label not in TRACKING_DATA_CORE_SIGNALS:
        raise HTTPException(status_code=400, detail="sensor_input_label not allowed for tracking source")
    dsn = ctx.dsn
    series: list[dict] = []
    with get_conn(dsn) as conn:
        cur = conn.execute(
            "SELECT EXISTS (SELECT 1 FROM pg_proc p JOIN pg_namespace n ON p.pronamespace = n.oid WHERE n.nspname = 'timescale' AND p.proname = 'time_bucket') AS has_tb"
        )
        row_tb = cur.fetchone()
        has_tb = bool(row_tb and row_tb.get("has_tb"))
        bucket_expr = "time_bucket('1 minute', device_time)" if has_tb else "date_trunc('minute', device_time)"

        if src == "input":
            sql = f"""
                SELECT {bucket_expr.replace('device_time', 'i.device_time')} AS bucket_ts,
                       avg(NULLIF(i.value,'')::numeric) AS value
                FROM raw_telematics_data.inputs i
                WHERE i.device_id = %s AND i.sensor_name = %s
                  AND i.device_time >= now() - make_interval(hours => %s)
                GROUP BY {bucket_expr.replace('device_time', 'i.device_time')}
                ORDER BY bucket_ts
            """
            cur = conn.execute(sql, (body.device_id, body.sensor_input_label, hours))
        elif src == "state":
            sql = f"""
                SELECT {bucket_expr.replace('device_time', 's.device_time')} AS bucket_ts,
                       avg(NULLIF(s.value,'')::numeric) AS value
                FROM raw_telematics_data.states s
                WHERE s.device_id = %s AND s.state_name = %s
                  AND s.device_time >= now() - make_interval(hours => %s)
                GROUP BY {bucket_expr.replace('device_time', 's.device_time')}
                ORDER BY bucket_ts
            """
            cur = conn.execute(sql, (body.device_id, body.sensor_input_label, hours))
        else:
            col = body.sensor_input_label
            sql = f"""
                SELECT {bucket_expr.replace('device_time', 't.device_time')} AS bucket_ts,
                       avg((t.{col})::numeric) AS value
                FROM raw_telematics_data.tracking_data_core t
                WHERE t.device_id = %s AND t.device_time >= now() - make_interval(hours => %s)
                GROUP BY {bucket_expr.replace('device_time', 't.device_time')}
                ORDER BY bucket_ts
            """
            cur = conn.execute(sql, (body.device_id, hours))

        for r in cur.fetchall():
            series.append({
                "ts": r["bucket_ts"].isoformat() if hasattr(r["bucket_ts"], "isoformat") else str(r["bucket_ts"]),
                "value": float(r["value"]) if r["value"] is not None else None,
            })
    return {"series": series}


# ---------- Dashboard planes ----------

@app.get("/api/dashboard-planes")
def list_dashboard_planes(ctx: RequestContext = Depends(_request_context)):
    uid = ctx.user_id
    dsn = ctx.dsn
    dp = app_state_table("dashboard_planes")
    cfg = app_state_table("configured_sensors")
    use_sqlite = ctx.app_state_dsn is None and app_state_uses_sqlite()
    is_active = "1" if use_sqlite else "true"
    try:
        with get_app_state_conn(dsn, override_dsn=ctx.app_state_dsn) as conn:
            if use_sqlite:
                cur = conn.execute(
                    f"""
                    SELECT d.dashboard_plane_id, d.configured_sensor_id, d.position_index,
                           c.object_id, c.device_id, c.sensor_input_label, c.sensor_source, c.sensor_label_custom,
                           c.min_threshold, c.max_threshold, c.multiplier
                    FROM {dp} d
                    JOIN {cfg} c ON c.configured_sensor_id = d.configured_sensor_id AND c.is_active = {is_active}
                    WHERE d.user_id = %s
                    ORDER BY d.position_index, d.dashboard_plane_id
                    """,
                    (uid,),
                )
                rows = cur.fetchall()
                if not rows:
                    return []
                object_ids = list({r["object_id"] for r in rows})
                with get_conn(dsn) as pg:
                    cur2 = pg.execute(
                        "SELECT object_id, object_label FROM raw_business_data.objects WHERE object_id = ANY(%s)",
                        (object_ids,),
                    )
                    labels = {r["object_id"]: r["object_label"] for r in cur2.fetchall()}
                for r in rows:
                    r["object_label"] = labels.get(r["object_id"])
                return [dict(r) for r in rows]
            try:
                cur = conn.execute(
                    f"""
                    SELECT d.dashboard_plane_id, d.configured_sensor_id, d.position_index,
                           c.object_id, c.device_id, c.sensor_input_label, c.sensor_source, c.sensor_label_custom,
                           c.min_threshold, c.max_threshold, c.multiplier,
                           o.object_label
                    FROM {dp} d
                    JOIN {cfg} c ON c.configured_sensor_id = d.configured_sensor_id AND c.is_active = true
                    JOIN raw_business_data.objects o ON o.object_id = c.object_id
                    WHERE d.user_id = %s
                    ORDER BY d.position_index, d.dashboard_plane_id
                    """,
                    (uid,),
                )
                rows = cur.fetchall()
            except psycopg.errors.UndefinedColumn:
                cur = conn.execute(
                    f"""
                    SELECT d.dashboard_plane_id, d.configured_sensor_id, d.position_index,
                           c.object_id, c.device_id, c.sensor_input_label, c.sensor_label_custom,
                           c.min_threshold, c.max_threshold,
                           o.object_label
                    FROM {dp} d
                    JOIN {cfg} c ON c.configured_sensor_id = d.configured_sensor_id AND c.is_active = true
                    JOIN raw_business_data.objects o ON o.object_id = c.object_id
                    WHERE d.user_id = %s
                    ORDER BY d.position_index, d.dashboard_plane_id
                    """,
                    (uid,),
                )
                rows = cur.fetchall()
                rows = [{**dict(r), "sensor_source": "input", "multiplier": None} for r in rows]
            return [dict(r) for r in rows]
    except psycopg.errors.UndefinedTable:
        return []


@app.post("/api/dashboard-planes")
def add_dashboard_plane(
    body: DashboardPlaneCreate,
    ctx: RequestContext = Depends(_request_context),
):
    uid = ctx.user_id
    dsn = ctx.dsn
    dp = app_state_table("dashboard_planes")
    cfg = app_state_table("configured_sensors")
    is_active = "1" if app_state_uses_sqlite() else "true"
    with get_app_state_conn(dsn, override_dsn=ctx.app_state_dsn) as conn:
        cur = conn.execute(
            f"SELECT 1 FROM {cfg} WHERE configured_sensor_id = %s AND user_id = %s AND is_active = {is_active}",
            (body.configured_sensor_id, uid),
        )
        if cur.fetchone() is None:
            raise HTTPException(403, "Configured sensor not found or access denied")
        cur = conn.execute(
            f"""
            INSERT INTO {dp} (user_id, configured_sensor_id, position_index)
            VALUES (%s, %s, %s)
            ON CONFLICT (user_id, configured_sensor_id) DO UPDATE SET position_index = EXCLUDED.position_index
            RETURNING dashboard_plane_id, configured_sensor_id, position_index
            """,
            (uid, body.configured_sensor_id, body.position_index),
        )
        row = cur.fetchone()
        conn.commit()
    return dict(row)


@app.delete("/api/dashboard-planes/{dashboard_plane_id:int}")
def remove_dashboard_plane(
    dashboard_plane_id: int,
    ctx: RequestContext = Depends(_request_context),
):
    uid = ctx.user_id
    dsn = ctx.dsn
    dp = app_state_table("dashboard_planes")
    with get_app_state_conn(dsn, override_dsn=ctx.app_state_dsn) as conn:
        cur = conn.execute(
            f"DELETE FROM {dp} WHERE dashboard_plane_id = %s AND user_id = %s",
            (dashboard_plane_id, uid),
        )
        conn.commit()
        if cur.rowcount == 0:
            raise HTTPException(404, "Dashboard plane not found")
    return {"ok": True}


@app.patch("/api/dashboard-planes/order")
def reorder_dashboard_planes(
    body: DashboardOrderRequest,
    ctx: RequestContext = Depends(_request_context),
):
    """Body: { "order": [ { "dashboard_plane_id": 1, "position_index": 0 }, ... ] }"""
    uid = ctx.user_id
    order = body.order or []
    dsn = ctx.dsn
    dp = app_state_table("dashboard_planes")
    with get_app_state_conn(dsn, override_dsn=ctx.app_state_dsn) as conn:
        for item in order:
            pid = item.get("dashboard_plane_id")
            idx = item.get("position_index", 0)
            if pid is not None:
                conn.execute(
                    f"UPDATE {dp} SET position_index = %s WHERE dashboard_plane_id = %s AND user_id = %s",
                    (idx, pid, uid),
                )
        conn.commit()
    return {"ok": True}


# ---------- Latest value (for dashboard indicators) ----------

@app.post("/api/latest-values")
def batch_latest_values(
    body: LatestValuesRequest,
    ctx: RequestContext = Depends(_request_context),
):
    """Body: { "pairs": [ { "device_id", "sensor_input_label", "sensor_source"?: "input"|"state"|"tracking" }, ... ] }
    Returns: { "values": { "device_id:source:sensor_input_label": { "value", "ts" } } }
    """
    pairs = body.pairs or []
    if not pairs:
        return {"values": {}}
    normalized = []
    for p in pairs:
        src = (p.get("sensor_source") or "input").strip().lower()
        if src not in ("input", "state", "tracking"):
            src = "input"
        normalized.append((p["device_id"], p["sensor_input_label"], src))
    dsn = ctx.dsn
    values: dict[str, dict] = {}
    with get_conn(dsn) as conn:
        input_keys = [(d, l) for (d, l, s) in normalized if s == "input"]
        if input_keys:
            placeholders = ",".join(["(%s,%s)"] * len(input_keys))
            flat = [x for k in input_keys for x in k]
            cur = conn.execute(
                f"""
                WITH cfg(device_id, sensor_name) AS (VALUES {placeholders}),
                latest AS (
                    SELECT DISTINCT ON (i.device_id, i.sensor_name)
                        i.device_id, i.sensor_name, i.device_time AS ts, NULLIF(i.value,'')::numeric AS value
                    FROM raw_telematics_data.inputs i
                    JOIN cfg ON cfg.device_id = i.device_id AND cfg.sensor_name = i.sensor_name
                    ORDER BY i.device_id, i.sensor_name, i.device_time DESC
                )
                SELECT device_id, sensor_name, ts, value FROM latest
                """,
                flat,
            )
            for r in cur.fetchall():
                key = _series_key(r["device_id"], r["sensor_name"], "input")
                values[key] = {"value": float(r["value"]) if r["value"] is not None else None, "ts": r["ts"].isoformat() if hasattr(r["ts"], "isoformat") else str(r["ts"])}
        state_keys = [(d, l) for (d, l, s) in normalized if s == "state"]
        if state_keys:
            placeholders = ",".join(["(%s,%s)"] * len(state_keys))
            flat = [x for k in state_keys for x in k]
            cur = conn.execute(
                f"""
                WITH cfg(device_id, state_name) AS (VALUES {placeholders}),
                latest AS (
                    SELECT DISTINCT ON (s.device_id, s.state_name)
                        s.device_id, s.state_name AS sensor_name, s.device_time AS ts, NULLIF(s.value,'')::numeric AS value
                    FROM raw_telematics_data.states s
                    JOIN cfg ON cfg.device_id = s.device_id AND cfg.state_name = s.state_name
                    ORDER BY s.device_id, s.state_name, s.device_time DESC
                )
                SELECT device_id, sensor_name, ts, value FROM latest
                """,
                flat,
            )
            for r in cur.fetchall():
                key = _series_key(r["device_id"], r["sensor_name"], "state")
                values[key] = {"value": float(r["value"]) if r["value"] is not None else None, "ts": r["ts"].isoformat() if hasattr(r["ts"], "isoformat") else str(r["ts"])}
        tracking_pairs = [(d, l) for (d, l, s) in normalized if s == "tracking" and l in TRACKING_DATA_CORE_SIGNALS]
        if tracking_pairs:
            for (device_id, col) in tracking_pairs:
                cur = conn.execute(
                    f"""
                    SELECT device_id, device_time AS ts, {col}::numeric AS value
                    FROM raw_telematics_data.tracking_data_core
                    WHERE device_id = %s
                    ORDER BY device_time DESC
                    LIMIT 1
                    """,
                    (device_id,),
                )
                r = cur.fetchone()
                if r:
                    key = _series_key(r["device_id"], col, "tracking")
                    values[key] = {"value": float(r["value"]) if r["value"] is not None else None, "ts": r["ts"].isoformat() if hasattr(r["ts"], "isoformat") else str(r["ts"])}
    return {"values": values}


# ---------- Serve frontend GUI when backend/static exists (e.g. single-URL deploy on Render) ----------

if _SERVE_GUI:
    _assets_dir = _STATIC_DIR / "assets"
    if _assets_dir.exists():
        app.mount("/assets", StaticFiles(directory=str(_assets_dir)), name="assets")

    @app.get("/")
    def _serve_index():
        return FileResponse(str(_STATIC_DIR / "index.html"))

    @app.get("/{full_path:path}")
    def _serve_spa(full_path: str):
        if full_path in ("docs", "redoc", "openapi.json") or full_path.startswith(("api/", "docs/", "redoc/")):
            raise HTTPException(status_code=404, detail="Not Found")
        return FileResponse(str(_STATIC_DIR / "index.html"))
else:
    @app.get("/")
    def root():
        return {
            "name": "Sensoriqua API",
            "docs": "/docs",
            "message": "This is the API. Deploy with frontend built into backend/static to get the GUI at this URL.",
        }
